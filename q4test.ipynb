{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup and imports for question 4\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# sets seed for reproducibility\n",
    "torch.manual_seed(250)\n",
    "torch.cuda.manual_seed(250)\n",
    "np.random.seed(250)\n",
    "\n",
    "ROOT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "CUDA = False\n",
    "DATA_PATH = './data'\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "lr = 1e-4\n",
    "classes = 4\n",
    "channels = 1\n",
    "img_size = 48\n",
    "latent_dim = 100\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGANImageDataset(Dataset):\n",
    "    '''Simple custom dataset to load in the 4 tree classes without the background class.'''\n",
    "    def __init__(self, root, image_dirs, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.all_images = []\n",
    "        self.labels = []\n",
    "        i = 0\n",
    "        for dir in image_dirs:\n",
    "            dir_path = root + \"/\" + image_dirs[i]\n",
    "            self.all_images += [img for img in os.listdir(dir_path) if img.endswith(\".tif\")]\n",
    "            self.labels += [i for img in os.listdir(dir_path) if img.endswith(\".tif\")]\n",
    "            i+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx] \n",
    "        image = Image.open(self.root + \"/class_\" + str(label+1) + \"/\" + self.all_images[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "CGAN_data = CGANImageDataset(ROOT + 'images', [\"class_1\", \"class_2\", \"class_3\", \"class_4\"], transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), \n",
    "                                                                                transforms.Normalize(0.5,0.5), transforms.Resize(img_size)]))                                                      \n",
    "\n",
    "cgan_y_train = CGAN_data.labels\n",
    "num_class_gan_train = Counter(cgan_y_train)\n",
    "cgan_class_sample_count = np.array([v for _, v in sorted(num_class_gan_train.items())])\n",
    "cgan_weight = 1. / torch.tensor(cgan_class_sample_count).float()\n",
    "cgan_samples_weight = np.array([cgan_weight[t] for t in cgan_y_train])\n",
    "cgan_samples_weight = torch.from_numpy(cgan_samples_weight)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(cgan_samples_weight, len(cgan_samples_weight))\n",
    "CGAN_dataloader=torch.utils.data.DataLoader(CGAN_data, batch_size=128, sampler = sampler, drop_last=True)\n",
    "\n",
    "# Prints and example of a batch\n",
    "images, labels = next(iter(CGAN_dataloader))\n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.8)\n",
    "cols, rows = 8, 8\n",
    "for i in range(cols * rows):\n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.title(labels[i].item())\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i,:].squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "if CUDA:\n",
    "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, classes, channels, img_size, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.channels = channels\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._create_layer(self.latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer(128, 256),\n",
    "            *self._create_layer(256, 512),\n",
    "            *self._create_layer(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _create_layer(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        z = torch.cat((self.label_embedding(labels), noise), -1)\n",
    "        x = self.model(z)\n",
    "        x = x.view(x.size(0), *self.img_shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, classes, channels, img_size, latent_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.channels = channels\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "        self.adv_loss = torch.nn.BCELoss()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._create_layer(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
    "            *self._create_layer(1024, 512, True, True),\n",
    "            *self._create_layer(512, 256, True, True),\n",
    "            *self._create_layer(256, 128, False, False),\n",
    "            *self._create_layer(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _create_layer(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.4))\n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, image, labels):\n",
    "        x = torch.cat((image.view(image.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the generator and the discriminator\n",
    "netG = Generator(classes, channels, img_size, latent_dim).to(device)\n",
    "print(netG)\n",
    "netD = Discriminator(classes, channels, img_size, latent_dim).to(device)\n",
    "print(netD)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optim_D = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optim_G = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "from torch.autograd import Variable\n",
    "img_list = []\n",
    "\n",
    "netG.train()\n",
    "netD.train()\n",
    "viz_z = torch.zeros((batch_size, latent_dim), device=device)\n",
    "viz_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "nrows = batch_size // 8\n",
    "viz_label = torch.LongTensor(np.array([num for _ in range(nrows) for num in range(8)])).to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(CGAN_dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        batch_size = data.size(0)\n",
    "        real_label = torch.full((batch_size, 1), 1., device=device)\n",
    "        fake_label = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "        # Train G\n",
    "        netG.zero_grad()\n",
    "        z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "        x_fake_labels = torch.randint(0, classes, (batch_size,), device=device)\n",
    "        x_fake = netG(z_noise, x_fake_labels)\n",
    "        y_fake_g = netD(x_fake, x_fake_labels)\n",
    "        g_loss = netD.loss(y_fake_g, real_label)\n",
    "        g_loss.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "        # Train D\n",
    "        netD.zero_grad()\n",
    "        y_real = netD(data, target)\n",
    "        d_real_loss = netD.loss(y_real, real_label)\n",
    "        y_fake_d = netD(x_fake.detach(), x_fake_labels)\n",
    "        d_fake_loss = netD.loss(y_fake_d, fake_label)\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optim_D.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f}'.format(\n",
    "                        epoch, batch_idx, len(CGAN_dataloader),\n",
    "                        d_loss.mean().item(),\n",
    "                        g_loss.mean().item()))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                viz_sample = netG(viz_noise, viz_label)\n",
    "                img_list.append(vutils.make_grid(viz_sample, normalize=True))\n",
    "\n",
    "    # Prints an example image from each of the 4 classes\n",
    "    z = Variable(torch.randn(4,100)).to(device)\n",
    "    labels = Variable(torch.LongTensor(np.arange(4))).to(device)\n",
    "    plt.figure(figsize=[3, 3])\n",
    "    sample_images = netG(z, labels)\n",
    "    grid = vutils.make_grid(sample_images, nrow=4, normalize=True).permute(1,2,0).numpy()\n",
    "    plt.imshow(grid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "netG.eval()\n",
    "\n",
    "p = 1\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "cols, rows = 4, 4\n",
    "for label in range(rows):\n",
    "    z = torch.randn(cols, 100, device=device)\n",
    "    labels = torch.LongTensor(np.array([label for _ in range(cols)])).to(device)\n",
    "    images = netG(z,labels)\n",
    "    for image in images:\n",
    "        figure.add_subplot(rows, cols, p)\n",
    "        plt.imshow(image.cpu().detach().squeeze().reshape(48, 48), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "        p+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0375c89bbc3c2e937e8ac87658b48ede521575fd3b0d3205cc7c280368c6f530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
