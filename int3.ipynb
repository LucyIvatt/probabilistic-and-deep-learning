{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems 3: Probabilistic and Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Regression and classification (42 marks)\n",
    "**a. Train and evaluate a least squares linear regression model predicting the value of\n",
    "variable D from variables A, B and C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is needed to use matplotlib in Jupyter notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Imports the data for question 1 and 2\n",
    "DATA = data = np.genfromtxt('data.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "def split_data(data, train_size):\n",
    "    # Shuffles the data but always in the same way to valid comparisons of models\n",
    "    np.random.seed(10)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # separates D as the target variable\n",
    "    X, y = data[:,:-1], data[:,-1]\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test = X[:-train_size], X[-train_size:]\n",
    "    y_train, y_test = y[:-train_size], y[-train_size:]\n",
    "    return X_train, X_test, y_train, y_test, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_regression(data, train_size, v=0, log=True):\n",
    "    \"\"\"Runs linear least squares regression on the data. \n",
    "    Normalises and scales the data if specified.\n",
    "    v=0 no preprocessing, v=1 normalise, v=2 scale, v=3 normalise and scale.\"\"\"\n",
    "\n",
    "    # Creates a copy of the data so the original data is not modified by the function\n",
    "    data = deepcopy(data)\n",
    "    \n",
    "    match v:\n",
    "        case 1 | 3:\n",
    "            data = normalize(data, axis=0) # axis=0 normalises the data by feature\n",
    "        case 2 | 3:\n",
    "            data = scale(data)\n",
    "    \n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    # Train model and make predictions\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(regr, X, y, cv=10)\n",
    "\n",
    "    # Prints the results\n",
    "    if log:\n",
    "        print(\"Coefficients of A, B & C:\", regr.coef_)\n",
    "        print('Mean squared error: %.2f'\n",
    "            % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f'\n",
    "            % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return regr, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (a) \" + \"-\" * 32)\n",
    "least_squares_regression(data=DATA, train_size=75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Repeat the above task after carrying out in turn data normalisation, data scaling and\n",
    "their combination, and evaluate the benefits of each of these 3 types of data preprocessing.**\n",
    "\n",
    "Below show the affect of data normalisation, data scaling and the combination of the two before performing the previous linear regression. Normalisation is useful where there are no outliers as it cannot cope with them, however standardisation is not affected by outliers as it is not bound by a certain range. \n",
    "\n",
    "[Normalisation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html): \n",
    "- Scale input vectors individually to unit norm (vector length)\n",
    "\n",
    "[Scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale):\n",
    "- Standardize a dataset along any axis. Center to the mean and component wise scale to unit variance.()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 32 + \" QUESTION 1 (b) \" + \"-\" * 32)\n",
    "\n",
    "# 1. Performs Data Normalisation\n",
    "print(\"Data Normalisation:\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=1)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 2. Performs Data Scaling\n",
    "print(\"Data scaling\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=2)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 3. Performs Data Normalisation and Data Scaling\n",
    "print(\"Norm then Scale\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=3)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Try to outperform the best result of the previous step by using regularisation (e.g. L1,\n",
    "L2 or Elastic Net). Show how any parameter values are tuned and evaluate the benefits of\n",
    "regularisation.**\n",
    "\n",
    "The use of regularisation has been implemented by using Ridge regression. The data was first pre-processed by standardising it - removing the mean and scaling to unit variance. The alpha value was determined by using cross-validation to determine the most successful value from the following list `[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def regularisation(data, train_size, log=True):\n",
    "    # Creates a copy of the data so the original data is not modified by the function\n",
    "    data = deepcopy(data)\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    pipe = Pipeline([('Scaler', StandardScaler()), ('Ridge', RidgeCV(alphas=np.logspace(-6, 6, 13)))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(pipe, X, y, cv=10)\n",
    "\n",
    "    if log:\n",
    "        print('Testing complexity parameter values (i.e.alphas): ', np.logspace(-6, 6, 13))\n",
    "        print('Cross-validation got this value for the complexity parameter: ', pipe.named_steps['Ridge'].alpha_)\n",
    "        for i, name in enumerate([\"A\", \"B\", \"C\"]):\n",
    "            print('Parameter for {0} is {1}'.format(name,pipe.named_steps['Ridge'].coef_[i]))\n",
    "        print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f' % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return pipe, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (c) \" + \"-\" * 32)\n",
    "regularisation(data=DATA, train_size=75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Add a set of suitable basis functions to the original data and train a linear regression\n",
    "with an appropriate type of regularisation to find out which of the new basis functions bring\n",
    "benefits. Explain briefly (in no more than 4 sentences) your reasoning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def polynomial_regression(data, train_size, log=True):\n",
    "    data = deepcopy(data)\n",
    "    \n",
    "    # splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    pipe = Pipeline([('Scaler', StandardScaler()), (\"BasisFunctions\", PolynomialFeatures(3)), (\"RidgeCV\", RidgeCV(alphas=np.logspace(-6, 6, 13)))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(pipe, X, y, cv=10)\n",
    "\n",
    "    if log:\n",
    "        print('Cross-validation got this value for the complexity parameter: ', pipe.named_steps['RidgeCV'].alpha_)\n",
    "        for i, name in enumerate([\"1 (bias)\", \"A\", \"B\", \"C\", \"A^2\", \"AB\", \"AC\", \"B^2\", \"BC\", \"C^2\", \"A^3\", \"(A^2)B\", \"(A^2)C\", \"A(B^2)\", \"ABC\", \"A(C^2)\", \"B^3\", \"(B^2)C\", \"B(C^2)\", \"C^3\"]):\n",
    "            print('Parameter for {0} is {1}'.format(name,pipe.named_steps['RidgeCV'].coef_[i]))\n",
    "        print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f' % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return pipe, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (d) \" + \"-\" * 32)\n",
    "polynomial_regression(DATA, 75)\n",
    "print(\"-\" * 76)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Implement an appropriate automated procedure that will train all of the above models\n",
    "and select the model expected to perform best on unseen data with the same distribution as your\n",
    "training data. You need to include a code tile at the end of this section of your Jupyter notebook\n",
    "that attempts to test your final choice of model on a data set stored in a file unseendata.csv\n",
    "and compute $R^2$ for it. The file will have exactly the same format as file data.csv, including\n",
    "the header, but possibly a different overall number of rows. This means you can use a renamed\n",
    "copy of data.csv to debug that part of your code, and to produce the corresponding content\n",
    "for your PDF file (in order to demonstrate that this part of the code is in working order).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- QUESTION 1 (e) --------------------------------\n",
      "1. Linear Regression has a mean r2 score of 0.87\n",
      "2. Linear Regression with Normalisation has a mean r2 score of 0.87\n",
      "3. Linear Regression with Standardisation has a mean r2 score of 0.87\n",
      "4. Regularisation has a mean r2 score of 0.87\n",
      "5. Linear regression with polynomial basis functions has a mean r2 score of 0.91\n",
      "\n",
      "The best model to perform on unseen data is Linear regression with polynomial basis functions as it has the highest mean r2 score of 0.91 after running 10-fold cross validation\n",
      "\n",
      "Running on unseendata.csv...\n",
      "Mean squared error: 0.21\n",
      "Coefficient of determination (R2): 0.91\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def choose_best_model(data, train_size):\n",
    "    \"\"\"Returns the model with the highest averaged cross-validation r2 score.\"\"\"\n",
    "    i, name, best_model, best_model_CV_mean = 1, None, None, None\n",
    "\n",
    "    for v in range(0, 3):\n",
    "        regr, cv_mean = least_squares_regression(data, train_size, v, log=False)\n",
    "        if v == 0:\n",
    "                mod_name = \"Linear Regression\"\n",
    "        elif v == 1:\n",
    "            mod_name = \"Linear Regression with Normalisation\"\n",
    "        elif v == 2:\n",
    "            mod_name = \"Linear Regression with Standardisation\"\n",
    "        elif v == 3: \n",
    "            mod_name = \"Linear regression with Normalisation and Standardisation\"\n",
    "        if best_model_CV_mean is None or cv_mean > best_model_CV_mean:\n",
    "            name, best_model, best_model_CV_mean = mod_name, regr, cv_mean\n",
    "        print(f\"{i}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "        i += 1\n",
    "            \n",
    "    regr, cv_mean = regularisation(data, train_size, log=False)\n",
    "    mod_name = \"Regularisation\"\n",
    "    if cv_mean > best_model_CV_mean:\n",
    "        best_model, best_model_CV_mean = regr, cv_mean\n",
    "        name = mod_name\n",
    "    print(f\"{4}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "    \n",
    "    regr, cv_mean = polynomial_regression(data, train_size, log=False)\n",
    "    mod_name = \"Linear regression with polynomial basis functions\"\n",
    "    if cv_mean > best_model_CV_mean:\n",
    "        best_model, best_model_CV_mean = regr, cv_mean\n",
    "        name = mod_name\n",
    "    print(f\"{5}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "    \n",
    "    print(\"\\nThe best model to perform on unseen data is \" + name + \" as it has the highest mean r2 score of \" + str(best_model_CV_mean.round(2)) + \" after running 10-fold cross validation\\n\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def test_unseen(best_model):\n",
    "    \"\"\"Runs the best model on the unseen data. Prints the mean squared error and R2 score.\"\"\"\n",
    "    unseen_data = np.genfromtxt('unseendata.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "    X, y = unseen_data[:,:-1], unseen_data[:,-1]\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Running on unseendata.csv...\")\n",
    "    print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_pred))\n",
    "    print('Coefficient of determination (R2): %.2f' % r2_score(y, y_pred))\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (e) \" + \"-\" * 32)\n",
    "best_model = choose_best_model(DATA, 75)\n",
    "test_unseen(best_model)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Starting with the data in data.csv, find the median value of variable D. Replace all\n",
    "values up to and including the median value with 0, and all values greater than that with 1. Treat\n",
    "the resulting values of D as class labels to train and evaluate a classifier based on logistic\n",
    "regression that takes variables A, B and C as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- QUESTION f (d) --------------------------------\n",
      "First 5 example predictions:\n",
      "Predicted class: 0.0 (Probability:0.9952)\n",
      "Predicted class: 0.0 (Probability:0.9999)\n",
      "Predicted class: 1.0 (Probability:0.8508)\n",
      "Predicted class: 1.0 (Probability:0.9999)\n",
      "Predicted class: 1.0 (Probability:0.9996)\n",
      "\n",
      "Out of sample accuracy = 88.0%\n",
      "\n",
      "Cross-Validation Mean Out of Sample Accuracy: 91.0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(data, train_size):\n",
    "    # Replaces the last column with 1 if the value is greater than the median, 0 otherwise.\n",
    "    median = np.median(data[:, 3])\n",
    "    data[:, 3] = np.where(data[:, 3] > median, 1, 0)\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    # Runs logistic regression\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "    predictions = logisticRegr.predict(X_test)\n",
    "    probability_predictions = logisticRegr.predict_proba(X_test)\n",
    "\n",
    "    # Shows first 5 predictions\n",
    "    print(\"First 5 example predictions:\")\n",
    "    for i in range(5):\n",
    "        print(\"Predicted class:\", predictions[i], \"(Probability:\" + str(probability_predictions[i][int(predictions[i])].round(4)) + \")\")\n",
    "\n",
    "     # Use score method to get out of sample accuracy of the model (2 decimal places)\n",
    "\n",
    "    score = logisticRegr.score(X_test, y_test)\n",
    "    print(\"\\nOut of sample accuracy = \" + str(score.round(2)*100) + \"%\")\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(logisticRegr, X, y, cv=10)\n",
    "\n",
    "    print(\"\\nCross-Validation Mean Out of Sample Accuracy: \" + str(scores.mean().round(2)*100) + \"%\")\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION f (d) \" + \"-\" * 32)\n",
    "logistic_regression(DATA, 75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Principal Component Analysis (8 marks)\n",
    "Starting with the same data.csv file from Q1, extend the table with 6 additional columns\n",
    "consisting of the product of each pair of the original 4 variables A, B, C and D.\n",
    "Apply principal component analysis (PCA) with a number of principal components (PCs) equal to\n",
    "the number of original variables, i.e. p = 4. Label the resulting principal components in\n",
    "decreasing order of variance as PC1. . .PC4 and list the linear equations showing how each of\n",
    "them is calculated from the 10 input variables. Describe which variables affect most strongly\n",
    "each of the 4 principal components, highlighting any notable findings and providing plausible\n",
    "explanations for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "\n",
    "data = np.loadtxt('data.csv',delimiter=',',skiprows=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "# extend the table with 6 additional column consisting of the product of each pair of the original 4 variables A, B, C and D\n",
    "for i, j in combinations(range(4), 2):\n",
    "    scaled_data = np.hstack((scaled_data, np.atleast_2d(np.multiply(scaled_data[:, i], scaled_data[:, j])).T))\n",
    "\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(scaled_data)\n",
    "newData = pca.fit_transform(scaled_data)\n",
    "\n",
    "NewTotVar = 0\n",
    "for i, dim in enumerate(('PC1','PC2','PC3','PC4')):\n",
    "  Var2 = np.var(newData[:,i])\n",
    "  NewTotVar += Var2\n",
    "\n",
    "  print('Here is the sample variance for the {0} dimension'.format(dim))\n",
    "  \n",
    "  print('var = {0}\\n\\n'.format(Var2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
