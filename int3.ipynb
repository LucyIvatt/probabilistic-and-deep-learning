{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [EXAM NUMBER] - Intelligent Systems 3: Probabilistic and Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The notebook expects that the files have been uploaded to the session storage so that relative paths can be used. The `_MACOSX` folder should be deleted after extracting the images for question 3 and 4 as it will cause issues with using `ImageFolder` to load the dataset. The following is the expected structure. \n",
    "\n",
    "This solution is assumed to be satisfactory as it was mentioned on the Assessment discussion board that session uploads and relative filepaths can be used.\n",
    "\n",
    "```\n",
    "├── images/\n",
    "│   ├── class_0/\n",
    "│   │   ├── 0000.tif\n",
    "│   │   └── ...\n",
    "│   ├── class_1/\n",
    "│   │   ├── 0000.tif\n",
    "│   │   └── ...\n",
    "│   ├── class_2/\n",
    "│   │   ├── 0000.tif\n",
    "│   │   └── ...\n",
    "│   ├── class_3/\n",
    "│   │   ├── 0000.tif\n",
    "│   │   └── ...\n",
    "│   └── class_4/\n",
    "│       ├── 0000.tif\n",
    "│       └── ...\n",
    "├── int3.ipynb\n",
    "├── data.csv\n",
    "├── unseendata.csv\n",
    "├── classify_symbols.py\n",
    "└── weights.pkl\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Regression and classification (42 marks)\n",
    "**a. Train and evaluate a least squares linear regression model predicting the value of\n",
    "variable D from variables A, B and C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports and setup for question 1\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.genfromtxt('data.csv', delimiter=',', skip_header=1, dtype=float) # Imports the data for question 1 and 2\n",
    "\n",
    "def split_data(data, train_size):\n",
    "    '''Shuffles the data and then splits into training and test sets.'''\n",
    "\n",
    "    # Shuffles the data but always in the same way to allow better comparisons of models\n",
    "    np.random.seed(10)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # separates D as the target variable\n",
    "    X, y = data[:,:-1], data[:,-1]\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test = X[:-train_size], X[-train_size:]\n",
    "    y_train, y_test = y[:-train_size], y[-train_size:]\n",
    "    return X_train, X_test, y_train, y_test, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_regression(data, train_size, v=0, log=True, plot=True):\n",
    "    \"\"\"Runs linear least squares regression on the data. \n",
    "    Normalises and scales the data if specified.\n",
    "    v=0 no preprocessing, v=1 normalise, v=2 scale, v=3 normalise and scale.\"\"\"\n",
    "\n",
    "    # Creates a copy of the data so the original data is not modified by the function\n",
    "    data_copy = deepcopy(data)\n",
    "    \n",
    "    if v == 1 or v == 3:\n",
    "            data_copy = normalize(data_copy, axis=0) # axis=0 normalises the data by feature\n",
    "\n",
    "    if v == 2 or v == 3:\n",
    "            data_copy = scale(data_copy)\n",
    "    \n",
    "    # Creates a graph of the (scaled) data if specified\n",
    "    if plot:\n",
    "        df = pd.DataFrame(data_copy, columns = ['A','B','C','D'])\n",
    "        df.plot()\n",
    "        plt.show()\n",
    "    \n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data_copy, train_size)\n",
    "\n",
    "    # Train model and make predictions\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(regr, X, y, cv=10)\n",
    "\n",
    "    # Prints the results\n",
    "    if log:\n",
    "        print(\"Coefficients of A, B & C:\", regr.coef_)\n",
    "        print('Mean squared error: %.2f'\n",
    "            % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f'\n",
    "            % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return regr, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (a) \" + \"-\" * 32)\n",
    "least_squares_regression(data=DATA, train_size=75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Repeat the above task after carrying out in turn data normalization, data scaling and\n",
    "their combination, and evaluate the benefits of each of these 3 types of data preprocessing.**\n",
    "\n",
    "_An assumption has been made that the normalization and scaling functions to be used should be the ones used in the week 5 hands on exercise \"Centering, normalizing and scaling\"._\n",
    "\n",
    "Below shows the affect of data normalization, data scaling and the combination of the two before performing the previous linear regression. \n",
    "\n",
    "- [Normalisation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html): Scale input vectors individually to unit norm (vector length)\n",
    "- [Scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale): Standardize a dataset along any axis. Center to the mean and component wise scale to unit variance.\n",
    "\n",
    "As shown by the R2 values for the three types of data preprocessing (and mean r2 values when running cross validation), none have had a significant impact on the efficiency of the model. This is likely due to the fact that its not necessary to normalize or scale the data when using linear regression as the coefficients learned by the model are not affected by the scale of the input features. \n",
    "\n",
    "However, standardizing the data can be useful as it makes it easier to interpret the coefficients that the model has learned. For instance, if one features scale is much larger than another's, then the coefficient for that feature may also be larger even if it does not affect the predicted value as strongly. Standardizing prevents this as all features are on the same scale. For example, it has shown that A, B and C all affect the value for D equally. We can prove this by plotting the original data vs the scaled data which is shown below alongside the coefficients and R2 values. \n",
    "\n",
    "Additionally as shown, if you perform data normalization and then scaling, it has the same effect as scaling the data and gives no further advantages than interpreting coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 32 + \" QUESTION 1 (b) \" + \"-\" * 32)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "df = pd.DataFrame(DATA, columns = ['A','B','C', 'D'])\n",
    "df.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 1. Performs Data Normalisation\n",
    "print(\"Data Normalisation:\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=1)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 2. Performs Data Scaling\n",
    "print(\"Data scaling\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=2)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 3. Performs Data Normalisation and Data Scaling\n",
    "print(\"Norm then Scale\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=3)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Try to outperform the best result of the previous step by using regularisation (e.g. L1,\n",
    "L2 or Elastic Net). Show how any parameter values are tuned and evaluate the benefits of\n",
    "regularisation.**\n",
    "\n",
    "The use of regularisation has been implemented by using RidgeCV regression. The data was first pre-processed by standardising it - removing the mean and scaling to unit variance. This is important as if features have different scales, then the penalty term will have a different effect on the coefficients of the features, depending on their scale. This can mean that the model may give more weight to features with larger scales, potentially ignoring smaller ones, unlike the previous linear regression.\n",
    "\n",
    "The alpha value was determined by using cross-validation to determine the most successful value from the following list `[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]`. The value chosen was 0.1.\n",
    "\n",
    "Comparing this to the previous regressions, this model has not performed any better, producing an identical R2 (and cross valided mean R2) value. As shown previously, we can see that the 3 features (A, B & C) are direct translations of each other. Its generally not a good idea to use highly correlated features in any type of regularized linear regression. The penalty term is not likely to be able to effectively determine any unimportant features as once scaled these features are identical (shown by the coefficients of A, B & C being identical). This therefore means that the addition of the penalty term had no additional benefit, and therefore has not been able to outperform the previous results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def regularisation(data, train_size, log=True):\n",
    "    # Creates a copy of the data so the original data is not modified by the function\n",
    "    data_copy = deepcopy(data)\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data_copy, train_size)\n",
    "\n",
    "    # Train model and make predictions (scales the data first and performs cross validation to find the best alpha value)\n",
    "    pipe = Pipeline([('Scaler', StandardScaler()), ('RidgeCV', RidgeCV(alphas=np.logspace(-6, 6, 13)))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(pipe, X, y, cv=10)\n",
    "\n",
    "    if log:\n",
    "        print('Testing the following alpha values for regularisation: ', np.logspace(-6, 6, 13))\n",
    "        print('Cross-validation got', pipe.named_steps['RidgeCV'].alpha_, 'for the complexity parameter')\n",
    "        for i, name in enumerate([\"A\", \"B\", \"C\"]):\n",
    "            print('Coefficient for {0} is {1}'.format(name,pipe.named_steps['RidgeCV'].coef_[i]))\n",
    "        print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f' % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return pipe, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (c) \" + \"-\" * 32)\n",
    "regularisation(data=DATA, train_size=75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Add a set of suitable basis functions to the original data and train a linear regression\n",
    "with an appropriate type of regularisation to find out which of the new basis functions bring\n",
    "benefits. Explain briefly (in no more than 4 sentences) your reasoning.**\n",
    "\n",
    "A set of basis functions has been added to the original data by using `PolynomialFeatures` with a degree of 3 to add all polynomial combinations of the features $<=$ to a degree of 3. After a process of trial and error 3 was selected as this produced the highest or equal r2 value. As previously mentioned, when scaled AB and C are all identical, making a lot of the basis functions added using PolynomialFeatures redundant ($A^2B$ is identical to $A^3$ etc.) and therefore only 3 coefficients have been identified from the model:\n",
    "\n",
    "- Linear (A, B, C) = 0.35\n",
    "- Squared (A^2, AB, etc.) = -0.0661\n",
    "- Cubed (A^3, ABC, etc.)= 0.0137\n",
    "\n",
    "The mean R2 score for this model is higher than the normal regularisation which suggests the small negative relationship for the squared functions and the small positive relationship of the cubed functions has slightly improved the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def polynomial_regression(data, train_size, log=True):\n",
    "    data = deepcopy(data)\n",
    "    \n",
    "    # splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    pipe = Pipeline([('Scaler', StandardScaler()), (\"BasisFunctions\", PolynomialFeatures(3)), (\"RidgeCV\", RidgeCV(alphas=np.logspace(-6, 6, 13)))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(pipe, X, y, cv=10)\n",
    "\n",
    "    if log:\n",
    "        print('Cross-validation got this value for the complexity parameter: ', pipe.named_steps['RidgeCV'].alpha_)\n",
    "        for i, name in enumerate([\"1 (bias)\", \"A\", \"B\", \"C\", \"A^2\", \"AB\", \"AC\", \"B^2\", \"BC\", \"C^2\", \"A^3\", \"(A^2)B\", \"(A^2)C\", \"A(B^2)\", \"ABC\", \"A(C^2)\", \"B^3\", \"(B^2)C\", \"B(C^2)\", \"C^3\"]):\n",
    "            print('Parameter for {0} is {1}'.format(name,pipe.named_steps['RidgeCV'].coef_[i].round(4)))\n",
    "        print('\\nMean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f' % r2_score(y_test, y_pred))\n",
    "        print(\"Cross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return pipe, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (d) \" + \"-\" * 32)\n",
    "polynomial_regression(DATA, 75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Implement an appropriate automated procedure that will train all of the above models\n",
    "and select the model expected to perform best on unseen data with the same distribution as your\n",
    "training data. You need to include a code tile at the end of this section of your Jupyter notebook\n",
    "that attempts to test your final choice of model on a data set stored in a file unseendata.csv\n",
    "and compute $R^2$ for it. The file will have exactly the same format as file data.csv, including\n",
    "the header, but possibly a different overall number of rows. This means you can use a renamed\n",
    "copy of data.csv to debug that part of your code, and to produce the corresponding content\n",
    "for your PDF file (in order to demonstrate that this part of the code is in working order).**\n",
    "\n",
    "This function takes the data as input and trains all of the models using `data.csv` and calculates their mean 10 fold cross validation scores. The model with the highest cross validation score has been chosen to train against the `unseendata.csv`.\n",
    "\n",
    "The model chosen was the `Linear regression with polynomial basis functions` with a mean R2 score of 0.91.\n",
    "\n",
    "_The unseen code block will be shown as 0.21 mean squared error and 0.91 R2 value as it was tested using a renamed version of `data.csv`._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_model(data, train_size):\n",
    "    \"\"\"Returns the model with the highest averaged cross-validation r2 score.\"\"\"\n",
    "    i, name, best_model, best_model_CV_mean = 1, None, None, None\n",
    "\n",
    "    for v in range(0, 3):\n",
    "        regr, cv_mean = least_squares_regression(data, train_size, v, log=False, plot=False)\n",
    "        if v == 0:\n",
    "                mod_name = \"Linear Regression\"\n",
    "        elif v == 1:\n",
    "            mod_name = \"Linear Regression with Normalisation\"\n",
    "        elif v == 2:\n",
    "            mod_name = \"Linear Regression with Standardisation\"\n",
    "        elif v == 3: \n",
    "            mod_name = \"Linear regression with Normalisation and Standardisation\"\n",
    "        if best_model_CV_mean is None or cv_mean > best_model_CV_mean:\n",
    "            name, best_model, best_model_CV_mean = mod_name, regr, cv_mean\n",
    "        print(f\"{i}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "        i += 1\n",
    "            \n",
    "    regr, cv_mean = regularisation(data, train_size, log=False)\n",
    "    mod_name = \"Regularisation\"\n",
    "    if cv_mean > best_model_CV_mean:\n",
    "        best_model, best_model_CV_mean = regr, cv_mean\n",
    "        name = mod_name\n",
    "    print(f\"{4}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "    \n",
    "    regr, cv_mean = polynomial_regression(data, train_size, log=False)\n",
    "    mod_name = \"Linear regression with polynomial basis functions\"\n",
    "    if cv_mean > best_model_CV_mean:\n",
    "        best_model, best_model_CV_mean = regr, cv_mean\n",
    "        name = mod_name\n",
    "    print(f\"{5}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "    \n",
    "    print(\"\\nThe best model to perform on unseen data is \" + name + \" as it has the highest mean r2 score of \" + str(best_model_CV_mean.round(2)) + \" after running 10-fold cross validation\\n\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def test_unseen(best_model):\n",
    "    \"\"\"Runs the best model on the unseen data. Prints the mean squared error and R2 score.\"\"\"\n",
    "    # Loads unseen data and splits into input and output values\n",
    "    unseen_data = np.genfromtxt('unseendata.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "    X, y = unseen_data[:,:-1], unseen_data[:,-1]\n",
    "\n",
    "    # Predicts the output values\n",
    "    y_pred = best_model.predict(X)\n",
    "    \n",
    "    print(\"Running on unseendata.csv...\")\n",
    "    print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_pred))\n",
    "    print('Coefficient of determination (R2): %.2f' % r2_score(y, y_pred))\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (e) \" + \"-\" * 32)\n",
    "best_model = choose_best_model(DATA, 75)\n",
    "test_unseen(best_model)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Starting with the data in data.csv, find the median value of variable D. Replace all\n",
    "values up to and including the median value with 0, and all values greater than that with 1. Treat\n",
    "the resulting values of D as class labels to train and evaluate a classifier based on logistic\n",
    "regression that takes variables A, B and C as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(data, train_size):\n",
    "    '''Replaces the last column with 1 if the value is greater than the median, 0 otherwise. \n",
    "    Trains a classifier using logistic regression using D as the class label.'''\n",
    "\n",
    "    # Replaces the last column with 1 if the value is greater than the median, 0 otherwise.\n",
    "    median = np.median(data[:, 3])\n",
    "    data[:, 3] = np.where(data[:, 3] > median, 1, 0)\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    # Runs logistic regression\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "    predictions = logisticRegr.predict(X_test)\n",
    "    probability_predictions = logisticRegr.predict_proba(X_test)\n",
    "\n",
    "    # Shows first 5 predictions as an example\n",
    "    print(\"First 5 example predictions:\")\n",
    "    for i in range(5):\n",
    "        print(\"Predicted class:\", predictions[i], \"(Probability:\" + str(probability_predictions[i][int(predictions[i])].round(4)) + \")\")\n",
    "\n",
    "     # Use score method to get out of sample accuracy of the model (2 decimal places)\n",
    "    score = logisticRegr.score(X_test, y_test)\n",
    "    print(\"\\nOut of sample accuracy = \" + str(score.round(2)*100) + \"%\")\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(logisticRegr, X, y, cv=10)\n",
    "\n",
    "    print(\"\\nCross-Validation Mean Out of Sample Accuracy: \" + str(scores.mean().round(2)*100) + \"%\")\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION f (d) \" + \"-\" * 32)\n",
    "logistic_regression(DATA, 75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Principal Component Analysis (8 marks)\n",
    "**Starting with the same data.csv file from Q1, extend the table with 6 additional columns\n",
    "consisting of the product of each pair of the original 4 variables A, B, C and D.\n",
    "Apply principal component analysis (PCA) with a number of principal components (PCs) equal to\n",
    "the number of original variables, i.e. p = 4. Label the resulting principal components in\n",
    "decreasing order of variance as PC1. . .PC4 and list the linear equations showing how each of\n",
    "them is calculated from the 10 input variables. Describe which variables affect most strongly\n",
    "each of the 4 principal components, highlighting any notable findings and providing plausible\n",
    "explanations for them.**\n",
    "\n",
    "PCA is affected by scale, therefore the data has been scaled using the standard scaler from sklearn. Similar to previous questions, the variables A, B and C may as well be equal once they are scaled. Therefore all coefficients relating to these three are similar.\n",
    "\n",
    "- PC1 - This component is most strongly affected by the products of D and the three other variables (AD, BD, CD) with a coefficient of 0.42. This suggests that as any of these variables increase so do the others. The products of A, B and C (AB, BC, AC) also have a coefficient of 0.35 which again suggests that as each of them increases so do the others. This is expected as we have already noted that A, B and C are all translations of the same data in the y direction. This does however note that D causes a slightly higher increase. This is a significant component and accounts for 59% of the variance.\n",
    "\n",
    "- PC2 - This component is most strongly affected by the basic variables A, B, C with identical coefficients of 0.48. D also has a coefficient of 0.43. This suggests that as A B and C increase, D also increases but slightly less. This is a significant component and accounts for 39% of the variance.\n",
    "\n",
    "- PC3 - This component is most strongly affected positively by variable D with a coefficient of 0.35, as well as all of the product variables of AB or C with a coefficient 0.4. Any product with D also significantly affects it negatively (-0.33). However the variance represented by this is only 1.5%.\n",
    "\n",
    "- PC4 - This component is most strongly affected positively by variable D with a coefficient of 0.8. However, this only represents 0.4% of the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "\n",
    "# loads the data and scales it\n",
    "data = np.loadtxt('data.csv',delimiter=',',skiprows=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "# extend the table with 6 additional column consisting of the product of each pair of the original 4 variables A, B, C and D\n",
    "# A, B, C, D, AB, AC, AD, BC, BD, CD\n",
    "for i, j in combinations(range(4), 2):\n",
    "    scaled_data = np.hstack((scaled_data, np.atleast_2d(np.multiply(scaled_data[:, i], scaled_data[:, j])).T))\n",
    "\n",
    "# performs PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(scaled_data)\n",
    "newData = pca.fit_transform(scaled_data)\n",
    "\n",
    "cols = [\"A\", \"B\", \"C\", \"D\", \"AB\", \"AC\", \"AD\", \"BC\", \"BD\", \"CD\"]\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "principal_components = pca.components_\n",
    "\n",
    "for i, dim in enumerate(('PC1','PC2','PC3','PC4')):\n",
    "  print('Sample variance for the {0} dimension'.format(dim))\n",
    "  print('var = {0}'.format(explained_variance[i]))\n",
    "  print('Linear Equation for the {0} dimension'.format(dim))\n",
    "  linear_eq = \"+\".join([\"(\" + \"{:.2f}\".format(coeff) + \"*\" + col + \")\" for coeff, col in zip(principal_components[i], cols)])\n",
    "  print(linear_eq)\n",
    "  print(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports any modules needed for question 3\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.optim import SGD\n",
    "from time import time\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "torch.manual_seed(100)\n",
    "torch.cuda.manual_seed(100)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into test/train datasets\n",
    "The model initially loads all of the images into the dataset using the `ImageFolder` class. It performs the simple operation of converting the images to grayscale to ensure that the number of channels for the images is 1 and then transforms them to pytorch tensors. \n",
    "\n",
    "As the dataset is unbalanced it needs to be split carefully to ensure that an equal proportion is in the test and train set. The following method was used to ensure this occurs. \n",
    "\n",
    "- Get the indices of every sample in the dataset\n",
    "- Performs a [stratified test/train split using sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) on the indices, stratifying on the class labels - 80:20 train/test split was used as this is a common best practice.\n",
    "- Uses the split indices to create pytorch `Subsets` of the original data.\n",
    "\n",
    "A bar chart showing the number of samples per subset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_split_stats(counts):\n",
    "    ''' Simple helper method to print the number of samples per class in each subset of the data.'''\n",
    "    total = sum(counts.values())\n",
    "    for k, v in sorted(counts.items()):\n",
    "        print(f\"Class {k} has {v} samples - {v/total*100:.2f}%\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Initialises \n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Number of samples of each class in the testing/training datasets, and the overall dataset.\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "\n",
    "# Loads the entire dataset, transforming images to grayscale and into pytorch tensors.\n",
    "data = datasets.ImageFolder(root='images', transform=transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]))\n",
    "total_num_classes = Counter(data.targets)\n",
    "print(\"Total data set: \", sum(total_num_classes.values()))\n",
    "print_split_stats(total_num_classes)\n",
    "plt.bar(total_num_classes.keys(), total_num_classes.values(), color='#003f5c')\n",
    "\n",
    "# Splits the data proportionally by \n",
    "indices = list(range(len(data)))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, stratify=data.targets)\n",
    "\n",
    "train_dataset = Subset(data, train_indices)\n",
    "num_class_train = Counter(data.targets[i] for i in train_indices)\n",
    "print(\"Train set: \", sum(num_class_train.values()))\n",
    "print_split_stats(num_class_train)\n",
    "plt.bar(num_class_train.keys(), num_class_train.values(), width=0.6, color='#58508d')\n",
    "\n",
    "test_dataset = Subset(data, test_indices)\n",
    "num_class_test = Counter(data.targets[i] for i in test_indices)\n",
    "print(\"Test set: \", sum(num_class_test.values()))\n",
    "print_split_stats(num_class_test)\n",
    "plt.bar(num_class_test.keys(), num_class_test.values(), width=0.4, color='#bc5090')\n",
    "\n",
    "overall_patch = mpatches.Patch(color='#003f5c', label='Overall Dataset')\n",
    "train_patch = mpatches.Patch(color='#58508d', label='Train Split')\n",
    "test_patch = mpatches.Patch(color='#bc5090', label='Test Split')\n",
    "\n",
    "plt.legend(handles=[overall_patch, train_patch, test_patch]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Batches\n",
    "The unbalanced dataset is also a problem when loading in each batch. Classes 1 and 2 have significantly less samples than the other classes in the dataset. This is important to mitigate as we don't want a training batch to contain samples just from 0, 3 and 4 and it instead should represent a good spread of the data.\n",
    "\n",
    "This has been achieved by using a `WeightedRandomSampler` in pytorch when defining the `DataLoaders`. The weights for each class label are inversely proportional to the number of samples for each class. This is then used within the training dataloader (not necessary for the test loader). The code for this is shown directly below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data.targets[i] for i in train_indices] # list of labels for the training set\n",
    "class_sample_count = np.array([v for _, v in sorted(num_class_train.items())]) # number of samples per class in the training set\n",
    "weight = 1. / torch.tensor(class_sample_count).float() # inverse proportion of the number of samples per class in the training set\n",
    "samples_weight = torch.from_numpy(np.array([weight[t] for t in y_train])) # sets the weight of each sample in the training set\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight)) # Defines the sampler using the calculated weights\n",
    "\n",
    "# Creates the dataloaders for the training and testing sets - uses the weighted sampler for the training set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows an example of a batch produced by the training data loader to prove the batches are now a better spread of the data. Along with the number of samples of each class in 5 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints first batch of data\n",
    "images, labels = next(iter(train_dataloader))\n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.8)\n",
    "cols, rows = 8, 8\n",
    "for i in range(cols * rows):\n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.title(labels[i].item())\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i,:].squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Creates a barchart showing the number of samples per class in each batch\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "labels = [\"Batch \" + str(i) for i in range(1, 6)]\n",
    "counts = [[] for i in range(5)]\n",
    "for i in range(5):\n",
    "    images, lab = next(iter(train_dataloader))\n",
    "    for k, v in Counter(lab.numpy()).items():\n",
    "        counts[k].append(v)\n",
    "\n",
    "colours = [\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#FA9566\"]\n",
    "n = 5\n",
    "ind = np.arange(n) # Position of bars on x-axis\n",
    "width = 0.15 # Width of a bar \n",
    "\n",
    "# Plotting\n",
    "for i in range(5):\n",
    "    plt.bar(ind + (width*i), counts[i] , width, label=labels[i], color=colours[i])\n",
    "\n",
    "plt.xlabel('Class Count')\n",
    "plt.ylabel('Batch Number')\n",
    "plt.title('Question 3 - Number of classes over 5 batches when using WeightedRandomSampler')\n",
    "plt.xticks(ind + ((width*3) / 2), labels)\n",
    "plt.legend(loc='best', labels=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure\n",
    "The network that has been chosen for the classifier is a version of a Convolutional Neural Network. These convolution layers are useful as they can help to extract features from the images that may be useful when classifying them, such as edges/textures. A CNN also allows for parameter sharing which means it should be more efficient when training and less prone to over fitting.  \n",
    "\n",
    "- 2 Convolution layers were used with kernel sizes of 3. The smaller layer size was used as it is often a popular choice for many reasons. Without an odd filter size there can be distortions across layers and bigger kernel sizes are much less cost efficient.. \n",
    "- After each convolution layer there are also BatchNorm layers. These have been added to reduce the internal covariate shift which should allow for more stable training and allow for the use of higher learning rates.\n",
    "- The reLU activation function has been used as it is relatively computationally efficient and simple. \n",
    "- MaxPooling has been used to reduce the number of parameters and therefore computation required in the network. Leaving the most prominent features from the previous convolution. \n",
    "- Finally a dropout of 0.2 has been included to help try and prevent over fitting which is more likely to occur due to the fewer number of unique samples for some of the classes. \n",
    "\n",
    "There are then 2 fully connected layers in the Multilayer perceptron ending in a logSoftmax to convert the output to a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.ConvolutionalLayers = nn.Sequential(\n",
    "            \n",
    "        # First convolutional layer\n",
    "        nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, stride=1, padding=1), # output size = B × 12 × 48 x 48\n",
    "        nn.BatchNorm2d(12),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # output size = B × 12 × 24 x 24\n",
    "        nn.Dropout(0.2),\n",
    "\n",
    "        # Second Convolutional layer\n",
    "        nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1), # output size = B × 24 × 24 x 24\n",
    "        nn.BatchNorm2d(24),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2), # output size = B × 24 × 12 x 12\n",
    "        nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "\t\tnn.Linear(in_features=12*12*24, out_features=300),\n",
    "        nn.BatchNorm1d(300),\n",
    "\t\tnn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "\n",
    "        nn.Linear(in_features=300, out_features=180),\n",
    "        nn.BatchNorm1d(180),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "\n",
    "\t\tnn.Linear(in_features=180, out_features=5),\n",
    "\t\tnn.LogSoftmax(dim=1) # probability distribution\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x has dimensions 64 x 1 x 48 x 48\n",
    "        x = self.ConvolutionalLayers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.MLP(x) # 64 x 5\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative log likelihood has been chosen for the loss function for the network. This does the same thing as CrossEntropyLoss however I have included the LogSoftMax within the network structure to convert the output into a probability distribution of the 5 classes. Either of these methods are the standard in multi-class classification problems.\n",
    "\n",
    "The optimiser chosen was the Stochastic Gradient Descent optimiser. This is a simple algorithm that may converge more slowly than Adam but is argued to generalise better. \n",
    "    - [_(Hardt, M., Recht, B., & Singer, Y. (2016, June). Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning (pp. 1225–1234). PMLR.)_](https://proceedings.mlr.press/v48/hardt16.html)\n",
    "\n",
    "The learning rate of 0.01 was chosen by starting with a relatively high value and gradually decreasing it whereas the momentum (0.9) started at a lower value and was increased until performance improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        train_loss = loss_func(output, labels)\n",
    "    \n",
    "        optimizer.zero_grad() # training\n",
    "        train_loss.backward() # learns by backprop\n",
    "        optimizer.step() # optimises weights\n",
    "\n",
    "        total += float(labels.size(0))\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = model(inputs)\n",
    "            test_loss = loss_func(output,labels)\n",
    "    \n",
    "    print('Epoch [{}/{}], Iteration [{}/{}], Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(e + 1, epochs, i + 1, len(train_dataloader), train_loss.item(), test_loss.item()))\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to save the weights of the model into a file\n",
    "torch.save(model.state_dict(), 'weights.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy\n",
    "Overall the model has an accuracy of 99.85% on the test set after making 3,361 correct predictions out of a total 3,366 guesses. The accuracy checking code is shown below along with an example of the images with their real and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    '''Uses the test dataset to check how many images the model correctly classifies'''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(f'{num_correct} / {num_samples} predictions correct with {float(num_correct)/float(num_samples)*100:.2f}% accuracy')\n",
    "\n",
    "check_accuracy(test_dataloader, model)\n",
    "\n",
    "# Run a test batch through the network\n",
    "images, labels = next(iter(test_dataloader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "output = model(images)\n",
    "pred_y = torch.argmax(output, 1)\n",
    "\n",
    "# Display first 25 images with predicted labels\n",
    "figure = plt.figure(figsize=(10, 12))\n",
    "for i in range(64):\n",
    "    figure.add_subplot(8, 8, i+1)\n",
    "    plt.title(\"real={} pred={}\".format(labels[i], pred_y[i]), fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i,:].squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Symbols Script\n",
    "Includes example code on inputting some images from the dataset and running them through the classify script to check everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classify_symbols\n",
    "\n",
    "image_tensors = []\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(num_output_channels=1), transforms.ToTensor()])\n",
    "\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    img = Image.open(\"images/class_\" + str(i) + \"/0000.tif\")\n",
    "    img = transform(img)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    image_tensors.append(img)\n",
    "\n",
    "images = torch.cat(image_tensors)\n",
    "\n",
    "output = classify_symbols.classify(images)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup and imports for question 4\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# sets seed for reproducibility\n",
    "torch.manual_seed(200)\n",
    "torch.cuda.manual_seed(200)\n",
    "np.random.seed(200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "As only classes 1-4 are used for generating the tree images, a custom dataset was defined to load in only these folders as `ImageFolder` which was used before has no parameters to ignore certain folders. \n",
    "\n",
    "CGANs usually train better when normalised, so an additional transformation was included to scale the images between -1 and 1.\n",
    "\n",
    "A `WeightedRandomSampler` was implemented in the same way as it previously was to make up for class imbalance. The stratified split for training/test sets is not needed as the entire dataset is used to train the CGAN. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "As the background class is not used for this question the ImageFolder class cannot as easily be used. Therefore a simple custom dataset has been created to only load in class 1, 2, 3 & 4. Additionally, due to the imbalance in the dataset the same mitigation of using WeightedRandomSampler has been utilised. However, the stratisfied sampling for test and training sets is not required as all images are used in the training for the CGAN.\n",
    "\n",
    "GANs also train better if the image data is normalised between -1 and 1 so a transform has been added to the dataset to perform this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGANImageDataset(Dataset):\n",
    "    '''Simple custom dataset to load in the 4 tree classes without the background class.'''\n",
    "    def __init__(self, root, image_dirs, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.all_images = []\n",
    "        self.labels = []\n",
    "        i = 0\n",
    "        for dir in image_dirs:\n",
    "            dir_path = root + \"/\" + image_dirs[i]\n",
    "            self.all_images += [img for img in os.listdir(dir_path) if img.endswith(\".tif\")]\n",
    "            self.labels += [i for img in os.listdir(dir_path) if img.endswith(\".tif\")]\n",
    "            i+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx] \n",
    "        image = Image.open(self.root + \"/class_\" + str(label+1) + \"/\" + self.all_images[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "CGAN_data = CGANImageDataset('images', [\"class_1\", \"class_2\", \"class_3\", \"class_4\"], transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), \n",
    "                                                                                transforms.Normalize(0.5,0.5)]))                                                      \n",
    "\n",
    "cgan_y_train = CGAN_data.labels\n",
    "num_class_gan_train = Counter(cgan_y_train)\n",
    "cgan_class_sample_count = np.array([v for _, v in sorted(num_class_gan_train.items())])\n",
    "cgan_weight = 1. / torch.tensor(cgan_class_sample_count).float()\n",
    "cgan_samples_weight = np.array([cgan_weight[t] for t in cgan_y_train])\n",
    "cgan_samples_weight = torch.from_numpy(cgan_samples_weight)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(cgan_samples_weight, len(cgan_samples_weight))\n",
    "CGAN_dataloader=torch.utils.data.DataLoader(CGAN_data, batch_size=BATCH_SIZE, sampler = sampler, drop_last=True)\n",
    "\n",
    "# Prints and example of a batch\n",
    "images, labels = next(iter(CGAN_dataloader))\n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.8)\n",
    "cols, rows = 8, 8\n",
    "for i in range(cols * rows):\n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.title(labels[i].item())\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i,:].squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "labels = [\"Batch \" + str(i) for i in range(1, 6)]\n",
    "counts = [[] for i in range(4)]\n",
    "for i in range(5):\n",
    "    images, lab = next(iter(CGAN_dataloader))\n",
    "    for k, v in Counter(lab.numpy()).items():\n",
    "        counts[k-1].append(v)\n",
    "\n",
    "colours = [\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\"]\n",
    "n = 5\n",
    "ind = np.arange(n) # Position of bars on x-axis\n",
    "width = 0.15  # Width of a bar \n",
    "\n",
    "for i in range(4):\n",
    "    plt.bar(ind + (width*i), counts[i] , width, label=labels[i], color=colours[i])\n",
    "\n",
    "plt.xlabel('Class Counts')\n",
    "plt.ylabel('Batch Number')\n",
    "plt.title('Question 4 - Number of classes over 5 batches when using WeightedRandomSampler')\n",
    "\n",
    "plt.xticks(ind + ((width*3) / 2), labels)\n",
    "plt.legend(loc='best', labels=[\"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Originally an architecture using convolution layers similar to DCGAN model was used. However, this only generated trees of one type which were all identical, and passing the label seemed to have no effect, even after altering the hyperparameters of the model.\n",
    "\n",
    "Therefore the simpler architecture shown below was designed as a test and produced better results. However, while it is able to generate an image of each of the 4 trees, it still suffers from severe mode collapse. \n",
    "\n",
    "An embedding layer has been used to turn the class label of 0, 1, 2 or 3 into a feature vector with a dimensionality of 40. This is then concatenated with the latent vector of size 100 and fed into the generator model. \n",
    "\n",
    "Unfortunately CGANs are difficult to train as both the generator model and the discriminator model are trained simultaneously in a zero sum game. This means that improvements to one model come at the expense of the other model. This has occured with the provided model and training is significantly unstable. After using a seed to allow the model to be reproducible, it was shown that at epoch 10 the model has learned about the 4 different labels but suffers from severe mode collapse, with the only differences being with the shading of some of the trees. \n",
    "\n",
    "LeakyReLU activation function has been used in both the generator and discriminator to help prevent a vanishing gradient.\n",
    "\n",
    "### Generator\n",
    "- Tanh has been used at the end of the generator model as the images are normalised between -1 and 1\n",
    "- Otherwise each layer is a fully connected linear layer\n",
    "\n",
    "### Discriminator\n",
    "- Utilises dropout to prevent over-fitting\n",
    "- Final sigmoid activation function as it is a binary classification\n",
    "\n",
    "The hyperparameters were again chosen by performing trial and error, and adjusting them based on the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_latent = 100 # Size of z latent vector (i.e. size of generator input)\n",
    "emb_num = 40 # Embedding layer output size\n",
    "num_epochs = 10 # Number of training epochs\n",
    "lr = 0.0001 # Learning rate for optimizers\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "dr = 0.3 # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedded_label = nn.Embedding(4,emb_num)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_latent + emb_num, 256), # x = torch.cat([z,c], axis=1) ## z.shape == (64,100), c.shape == (64,10), torch.cat([z,c], axis=1).shape == (64,110)\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(256, 768),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(768, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(1024, 2304),\n",
    "            nn.Tanh() # as normalised between -1 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), z_latent)\n",
    "        c = self.embedded_label(labels)\n",
    "        x = torch.cat([z,c], axis=1)\n",
    "        output = self.model(x)\n",
    "        return output.view(x.size(0), 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedded_label = nn.Embedding(4,emb_num)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2304 + emb_num, 1024), # x = torch.cat([x,c], axis=1) ## x.shape == (64,784), c.shape == (64,10), torch.cat([x,c],axis=1).shape == (64,794)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dr),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dr),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dr),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() # binary classification problem of real or fake\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        c = self.embedded_label(labels)\n",
    "        x = x.view(x.size(0), 2304)\n",
    "        x = torch.cat([x,c], axis=1)\n",
    "        output = self.model(x)\n",
    "        return output.squeeze()\n",
    "\n",
    "cond_gen = ConditionalGenerator().to(device)\n",
    "cond_discrim = ConditionalDiscriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Adam optimizers for generator and discriminator\n",
    "d_optimizer = torch.optim.Adam(cond_discrim.parameters(), lr = lr, betas=(beta1, 0.999))\n",
    "g_optimizer = torch.optim.Adam(cond_gen.parameters(), lr = lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "def train_generator(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    '''Trains the generator network'''\n",
    "    g_optimizer.zero_grad()\n",
    "    z = Variable(torch.randn(batch_size,z_latent)).to(device)\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 4, batch_size))).to(device)\n",
    "    fake_imgs = generator(z, fake_labels)\n",
    "    validity = discriminator(fake_imgs, fake_labels) \n",
    "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).to(device)).to(device) # get generators loss\n",
    "    g_loss.backward() # back prop\n",
    "    g_optimizer.step() # adjust model parameters\n",
    "    return g_loss.data\n",
    "\n",
    "def train_discriminator(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    '''Trains the discriminator network.'''\n",
    "    d_optimizer.zero_grad()\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).to(device))\n",
    "    z = Variable(torch.randn(batch_size, z_latent)).to(device)\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0,4,batch_size))).to(device)\n",
    "    fake_imgs = generator(z, fake_labels)\n",
    "    fake_validity = discriminator(fake_imgs, fake_labels)\n",
    "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).to(device))\n",
    "    d_loss = real_loss + fake_loss # discriminator loss\n",
    "    d_loss.backward() # back prop\n",
    "    d_optimizer.step() # adjust params\n",
    "    return d_loss.data\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch : {}'.format(epoch))\n",
    "    for _, (images, labels) in enumerate(CGAN_dataloader):\n",
    "        real_images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        cond_gen.train()\n",
    "        d_loss = train_discriminator(len(real_images), cond_discrim, cond_gen, d_optimizer, criterion, real_images, labels)\n",
    "        g_loss = train_generator(BATCH_SIZE, cond_discrim, cond_gen, g_optimizer, criterion)\n",
    "        \n",
    "    cond_gen.eval()\n",
    "    print('g_loss : {}, d_loss : {}'.format(g_loss, d_loss))\n",
    "    \n",
    "    # Prints an example image from each of the 4 classes\n",
    "    z = Variable(torch.randn(4,100)).to(device)\n",
    "    labels = Variable(torch.LongTensor(np.arange(4))).to(device)\n",
    "    plt.figure(figsize=[3, 3])\n",
    "    sample_images = cond_gen(z, labels).unsqueeze(1).data.cpu()\n",
    "    grid = make_grid(sample_images, nrow=4, normalize=True).permute(1,2,0).numpy()\n",
    "    plt.imshow(grid)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Images\n",
    "4 of each class - significantly affected by mode collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "cond_gen.eval()\n",
    "\n",
    "p = 1\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "cols, rows = 4, 4\n",
    "for label in range(rows):\n",
    "    z = torch.randn(cols, z_latent, device=device)\n",
    "    labels = torch.LongTensor(np.array([label for _ in range(cols)])).to(device)\n",
    "    images = cond_gen(z,labels)\n",
    "    for image in images:\n",
    "        figure.add_subplot(rows, cols, p)\n",
    "        plt.imshow(image.cpu().detach().squeeze().reshape(48, 48), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "        p+=1\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation between 2 of each class, difficult to see any effect due to mode collapse of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 7\n",
    "z1 = torch.randn(1, z_latent, 1, 1).to(device)\n",
    "z2 = torch.randn(1, z_latent, 1, 1).to(device)\n",
    "z = torch.zeros(nsamples,z_latent,1,1).to(device)\n",
    "for i in range(nsamples):\n",
    "  w1 = i/(nsamples-1)\n",
    "  w2 = 1-w1\n",
    "  z[i,:,:,:] = w1*z1 + w2*z2\n",
    "\n",
    "for tree in range(4):\n",
    "  x = torch.IntTensor([tree for i in range(7)]).to(device)\n",
    "  images = cond_gen(z, x).to(device)\n",
    "  figure = plt.figure(figsize=(12, 4))\n",
    "  for i in range(nsamples):\n",
    "      figure.add_subplot(1, nsamples, i+1)\n",
    "      plt.axis(\"off\")\n",
    "      plt.imshow(0.5-0.5*images[i,:].squeeze().cpu().detach(), cmap=\"gray_r\")\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0375c89bbc3c2e937e8ac87658b48ede521575fd3b0d3205cc7c280368c6f530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
