{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems 3: Probabilistic and Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Regression and classification (42 marks)\n",
    "**a. Train and evaluate a least squares linear regression model predicting the value of\n",
    "variable D from variables A, B and C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is needed to use matplotlib in Jupyter notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Imports the data for question 1 and 2\n",
    "DATA = data = np.genfromtxt('data.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "def split_data(data, train_size):\n",
    "    # Shuffles the data but always in the same way to valid comparisons of models\n",
    "    np.random.seed(10)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # separates D as the target variable\n",
    "    X, y = data[:,:-1], data[:,-1]\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test = X[:-train_size], X[-train_size:]\n",
    "    y_train, y_test = y[:-train_size], y[-train_size:]\n",
    "    return X_train, X_test, y_train, y_test, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def least_squares_regression(data, train_size, v=0, log=True):\n",
    "    \"\"\"Runs linear least squares regression on the data. \n",
    "    Normalises and scales the data if specified.\n",
    "    v=0 no preprocessing, v=1 normalise, v=2 scale, v=3 normalise and scale.\"\"\"\n",
    "\n",
    "    # Creates a copy of the data so the original data is not modified by the function\n",
    "    data = deepcopy(data)\n",
    "    \n",
    "    if v == 1 or v == 3:\n",
    "            data = normalize(data, axis=0) # axis=0 normalises the data by feature\n",
    "\n",
    "    if v == 2 or v == 3:\n",
    "            data = scale(data)\n",
    "\n",
    "    if v == 4 or v == 6:\n",
    "            minmax = MinMaxScaler()\n",
    "            data = minmax.fit_transform(data)\n",
    "\n",
    "    if v == 5 or v == 6:\n",
    "            scaler = StandardScaler()\n",
    "            data = scaler.fit_transform(data) \n",
    "    \n",
    "    df = pd.DataFrame(data, columns = ['A','B','C', 'D'])\n",
    "    df.plot()\n",
    "    \n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    # Train model and make predictions\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(regr, X, y, cv=10)\n",
    "\n",
    "    # Prints the results\n",
    "    if log:\n",
    "        print(\"Coefficients of A, B & C:\", regr.coef_)\n",
    "        print('Mean squared error: %.2f'\n",
    "            % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f'\n",
    "            % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return regr, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (a) \" + \"-\" * 32)\n",
    "least_squares_regression(data=DATA, train_size=75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Repeat the above task after carrying out in turn data normalisation, data scaling and\n",
    "their combination, and evaluate the benefits of each of these 3 types of data preprocessing.**\n",
    "\n",
    "Below shows the affect of data normalisation, data scaling and the combination of the two before performing the previous linear regression.\n",
    "\n",
    "[Normalisation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html): Scale input vectors individually to unit norm (vector length)\n",
    "\n",
    "[Scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale): Standardize a dataset along any axis. Center to the mean and component wise scale to unit variance.\n",
    "\n",
    "As shown by the R2 values for the three types of data preprocessing (and mean r2 values when running cross validation), none have had a significant impact on the efficiency of the model. This is likely due to the fact that its not neccessary to normalise or scale the data when using linear regression as the coefficients learned by the model are not affected by the scale of the input features. \n",
    "\n",
    "However, standardizing the data can be useful as it makes it easier to interpret the coefficients that the model has learned. For instance, if one features scale is much larger than anothers, then the coefficient for that feature may also be larger in comparison to other features with smaller scales. Standardizing prevents this as all features are on the same scale. For example, it has shown that A, B and C all affect the value for D equally. We can prove this by plotting the original data vs the scaled data which is shown below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 32 + \" QUESTION 1 (b) \" + \"-\" * 32)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['A','B','C', 'D'])\n",
    "df.plot()\n",
    "\n",
    "# 1. Performs Data Normalisation - using lecture example\n",
    "print(\"Data Normalisation:\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=1)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 2. Performs Data Scaling - using lecture example\n",
    "print(\"Data scaling\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=2)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 3. Performs Data Normalisation and Data Scaling - using lecture examples\n",
    "print(\"Norm then Scale\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=3)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 4. Performs Data Normalisation - using min max\n",
    "print(\"Norm - min max\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=4)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 3. Performs Scaling - using standard scaler\n",
    "print(\"Scale - standard scaler\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=5)\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# 3. Performs Data Normalisation and Data Scaling - using MinMaxScaler and StandardScaler\n",
    "print(\"Scale - min max then scaler\")\n",
    "least_squares_regression(data=DATA, train_size=75, v=6)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Try to outperform the best result of the previous step by using regularisation (e.g. L1,\n",
    "L2 or Elastic Net). Show how any parameter values are tuned and evaluate the benefits of\n",
    "regularisation.**\n",
    "\n",
    "The use of regularisation has been implemented by using RidgeCV regression. The data was first pre-processed by standardising it - removing the mean and scaling to unit variance. This is important as if features have different scales, then the penalty term will have a different effect on the coefficients of the features, depending on their scale. This can mean that the model may give more weight to features with larger scales, potentially ignoring smaller ones.\n",
    "\n",
    "The alpha value was determined by using cross-validation to determine the most successful value from the following list `[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]`. The value chosen was 0.1.\n",
    "\n",
    "In comparison to the previous regressions, this model has not performed any better, producing an identical R2 (and cross valided mean R2) value. As shown previously, we can see that the 3 features (A, B & C) are direct translations of each other. Its generally not a good idea to use highly correlated features in any type of regularized linear regression. The penalty term is not likely to be able to effectively determine any unimportant features as once scaled these features are identical (shown by the coefficients of A, B & C being identical). This therefore means that the addition of the penalty term had no additional effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def regularisation(data, train_size, log=True):\n",
    "    # Creates a copy of the data so the original data is not modified by the function\n",
    "    data = deepcopy(data)\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    pipe = Pipeline([('Scaler', StandardScaler()), ('RidgeCV', RidgeCV(alphas=np.logspace(-6, 6, 13)))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(pipe, X, y, cv=10)\n",
    "\n",
    "    if log:\n",
    "        print('Testing complexity parameter values (i.e.alphas): ', np.logspace(-6, 6, 13))\n",
    "        print('Cross-validation got this value for the complexity parameter: ', pipe.named_steps['RidgeCV'].alpha_)\n",
    "        for i, name in enumerate([\"A\", \"B\", \"C\"]):\n",
    "            print('Coefficient for {0} is {1}'.format(name,pipe.named_steps['RidgeCV'].coef_[i]))\n",
    "        print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f' % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return pipe, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (c) \" + \"-\" * 32)\n",
    "regularisation(data=DATA, train_size=75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Add a set of suitable basis functions to the original data and train a linear regression\n",
    "with an appropriate type of regularisation to find out which of the new basis functions bring\n",
    "benefits. Explain briefly (in no more than 4 sentences) your reasoning.**\n",
    "\n",
    "As previously mentioned, when scaled AB and C are all identical, making a lot of the basis functions added using PolynomialFeatures redundant. For example, $A^2B$ is identical to $A^3$. Therefore only 3 coefficients have been identified from the new basis functions which are:\n",
    "\n",
    "- Linear (A, B, C) = 0.35\n",
    "- Squared (A^2, AB, etc.) = -0.0661\n",
    "- Cubed (A^3, ABC, etc.)= 0.0137\n",
    "\n",
    "The mean R2 score for this model is higher than the normal regularisation. This suggests the small negative relationship for the squared functions and the small positive relationship of the cubed functions has slightly improved the predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def polynomial_regression(data, train_size, log=True):\n",
    "    data = deepcopy(data)\n",
    "    \n",
    "    # splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    pipe = Pipeline([('Scaler', StandardScaler()), (\"BasisFunctions\", PolynomialFeatures(3)), (\"RidgeCV\", RidgeCV(alphas=np.logspace(-6, 6, 13)))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(pipe, X, y, cv=10)\n",
    "\n",
    "    if log:\n",
    "        print('Cross-validation got this value for the complexity parameter: ', pipe.named_steps['RidgeCV'].alpha_)\n",
    "        for i, name in enumerate([\"1 (bias)\", \"A\", \"B\", \"C\", \"A^2\", \"AB\", \"AC\", \"B^2\", \"BC\", \"C^2\", \"A^3\", \"(A^2)B\", \"(A^2)C\", \"A(B^2)\", \"ABC\", \"A(C^2)\", \"B^3\", \"(B^2)C\", \"B(C^2)\", \"C^3\"]):\n",
    "            print('Parameter for {0} is {1}'.format(name,pipe.named_steps['RidgeCV'].coef_[i]))\n",
    "        print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination (R2): %.2f' % r2_score(y_test, y_pred))\n",
    "        print(\"\\nCross-Validation Mean R2 Score: \" + str(scores.mean().round(2)))\n",
    "\n",
    "    return pipe, scores.mean()\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (d) \" + \"-\" * 32)\n",
    "polynomial_regression(DATA, 75)\n",
    "print(\"-\" * 76)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Implement an appropriate automated procedure that will train all of the above models\n",
    "and select the model expected to perform best on unseen data with the same distribution as your\n",
    "training data. You need to include a code tile at the end of this section of your Jupyter notebook\n",
    "that attempts to test your final choice of model on a data set stored in a file unseendata.csv\n",
    "and compute $R^2$ for it. The file will have exactly the same format as file data.csv, including\n",
    "the header, but possibly a different overall number of rows. This means you can use a renamed\n",
    "copy of data.csv to debug that part of your code, and to produce the corresponding content\n",
    "for your PDF file (in order to demonstrate that this part of the code is in working order).**\n",
    "\n",
    "This function takes the data as input and trains all of the models and calculated their mean 10 fold cross validation scores. The model with the highest cross validation score has been chosen to train against the unseendata.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_model(data, train_size):\n",
    "    \"\"\"Returns the model with the highest averaged cross-validation r2 score.\"\"\"\n",
    "    i, name, best_model, best_model_CV_mean = 1, None, None, None\n",
    "\n",
    "    for v in range(0, 3):\n",
    "        regr, cv_mean = least_squares_regression(data, train_size, v, log=False)\n",
    "        if v == 0:\n",
    "                mod_name = \"Linear Regression\"\n",
    "        elif v == 1:\n",
    "            mod_name = \"Linear Regression with Normalisation\"\n",
    "        elif v == 2:\n",
    "            mod_name = \"Linear Regression with Standardisation\"\n",
    "        elif v == 3: \n",
    "            mod_name = \"Linear regression with Normalisation and Standardisation\"\n",
    "        if best_model_CV_mean is None or cv_mean > best_model_CV_mean:\n",
    "            name, best_model, best_model_CV_mean = mod_name, regr, cv_mean\n",
    "        print(f\"{i}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "        i += 1\n",
    "            \n",
    "    regr, cv_mean = regularisation(data, train_size, log=False)\n",
    "    mod_name = \"Regularisation\"\n",
    "    if cv_mean > best_model_CV_mean:\n",
    "        best_model, best_model_CV_mean = regr, cv_mean\n",
    "        name = mod_name\n",
    "    print(f\"{4}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "    \n",
    "    regr, cv_mean = polynomial_regression(data, train_size, log=False)\n",
    "    mod_name = \"Linear regression with polynomial basis functions\"\n",
    "    if cv_mean > best_model_CV_mean:\n",
    "        best_model, best_model_CV_mean = regr, cv_mean\n",
    "        name = mod_name\n",
    "    print(f\"{5}. {mod_name} has a mean r2 score of {cv_mean.round(2)}\")\n",
    "    \n",
    "    print(\"\\nThe best model to perform on unseen data is \" + name + \" as it has the highest mean r2 score of \" + str(best_model_CV_mean.round(2)) + \" after running 10-fold cross validation\\n\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def test_unseen(best_model):\n",
    "    \"\"\"Runs the best model on the unseen data. Prints the mean squared error and R2 score.\"\"\"\n",
    "    unseen_data = np.genfromtxt('unseendata.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "    X, y = unseen_data[:,:-1], unseen_data[:,-1]\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Running on unseendata.csv...\")\n",
    "    print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_pred))\n",
    "    print('Coefficient of determination (R2): %.2f' % r2_score(y, y_pred))\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION 1 (e) \" + \"-\" * 32)\n",
    "best_model = choose_best_model(DATA, 75)\n",
    "test_unseen(best_model)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Starting with the data in data.csv, find the median value of variable D. Replace all\n",
    "values up to and including the median value with 0, and all values greater than that with 1. Treat\n",
    "the resulting values of D as class labels to train and evaluate a classifier based on logistic\n",
    "regression that takes variables A, B and C as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(data, train_size):\n",
    "    # Replaces the last column with 1 if the value is greater than the median, 0 otherwise.\n",
    "    median = np.median(data[:, 3])\n",
    "    data[:, 3] = np.where(data[:, 3] > median, 1, 0)\n",
    "\n",
    "    # Splits the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, X, y = split_data(data, train_size)\n",
    "\n",
    "    # Runs logistic regression\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "    predictions = logisticRegr.predict(X_test)\n",
    "    probability_predictions = logisticRegr.predict_proba(X_test)\n",
    "\n",
    "    # Shows first 5 predictions\n",
    "    print(\"First 5 example predictions:\")\n",
    "    for i in range(5):\n",
    "        print(\"Predicted class:\", predictions[i], \"(Probability:\" + str(probability_predictions[i][int(predictions[i])].round(4)) + \")\")\n",
    "\n",
    "     # Use score method to get out of sample accuracy of the model (2 decimal places)\n",
    "\n",
    "    score = logisticRegr.score(X_test, y_test)\n",
    "    print(\"\\nOut of sample accuracy = \" + str(score.round(2)*100) + \"%\")\n",
    "\n",
    "    # Test of generalisation (10-fold cross-validation)\n",
    "    scores = cross_val_score(logisticRegr, X, y, cv=10)\n",
    "\n",
    "    print(\"\\nCross-Validation Mean Out of Sample Accuracy: \" + str(scores.mean().round(2)*100) + \"%\")\n",
    "\n",
    "print(\"-\" * 32 + \" QUESTION f (d) \" + \"-\" * 32)\n",
    "logistic_regression(DATA, 75)\n",
    "print(\"-\" * 76)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Principal Component Analysis (8 marks)\n",
    "**Starting with the same data.csv file from Q1, extend the table with 6 additional columns\n",
    "consisting of the product of each pair of the original 4 variables A, B, C and D.\n",
    "Apply principal component analysis (PCA) with a number of principal components (PCs) equal to\n",
    "the number of original variables, i.e. p = 4. Label the resulting principal components in\n",
    "decreasing order of variance as PC1. . .PC4 and list the linear equations showing how each of\n",
    "them is calculated from the 10 input variables. Describe which variables affect most strongly\n",
    "each of the 4 principal components, highlighting any notable findings and providing plausible\n",
    "explanations for them.**\n",
    "\n",
    "Similar to previous questions, the variables A, B and C may as well be equal as they have been scaled. Therefore all coefficients relating to these three are similar.\n",
    "\n",
    "- PC1\n",
    "- PC2\n",
    "- PC3 - This component is most strongly affected positively by variable D with a coefficient of 0.35, as well as all of the product variables of AB or C with a coefficient 0.4. Any product with D also significantly affects it negatively (-0.33). However the variance is only 1.5% for this variable.\n",
    "- PC4 - This component is most strongly affected positively by variable D with a coefficient of 0.8. However, this only represents 0.4%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "\n",
    "data = np.loadtxt('data.csv',delimiter=',',skiprows=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "# extend the table with 6 additional column consisting of the product of each pair of the original 4 variables A, B, C and D\n",
    "# A, B, C, D, AB, AC, AD, BC, BD, CD\n",
    "\n",
    "for i, j in combinations(range(4), 2):\n",
    "    scaled_data = np.hstack((scaled_data, np.atleast_2d(np.multiply(scaled_data[:, i], scaled_data[:, j])).T))\n",
    "\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(scaled_data)\n",
    "newData = pca.fit_transform(scaled_data)\n",
    "\n",
    "\n",
    "cols = [\"A\", \"B\", \"C\", \"D\", \"AB\", \"AC\", \"AD\", \"BC\", \"BD\", \"CD\"]\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "principal_components = pca.components_\n",
    "\n",
    "NewTotVar = 0\n",
    "for i, dim in enumerate(('PC1','PC2','PC3','PC4')):\n",
    "  print('Sample variance for the {0} dimension'.format(dim))\n",
    "  print('var = {0}'.format(explained_variance[i]))\n",
    "  print('Linear Equation for the {0} dimension'.format(dim))\n",
    "  linear_eq = \"+\".join([\"(\" + \"{:.2f}\".format(coeff) + \"*\" + col + \")\" for coeff, col in zip(principal_components[i], cols)])\n",
    "  print(linear_eq)\n",
    "  print(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "classification, multiclass -> unconstrainted  use softmax to convert to probability distribution\n",
    "compare estimated probability dis with correct one by using cross entropy loss\n",
    "black + white image means 2d tensor\n",
    "\n",
    "number of items = batch size first dimension of tensor\n",
    "channels or features are special dimension\n",
    "\n",
    "https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00\n",
    "\n",
    "https://medium.com/analytics-vidhya/augment-your-data-easily-with-pytorch-313f5808fc8b\n",
    "\n",
    "https://www.learnpytorch.io/04_pytorch_custom_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.optim import SGD\n",
    "from time import time\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into test/train datasets\n",
    "- unbalanced dataset so needs to be split carefully. \n",
    "- initially loads all images in using ImageFolder class, then this is analysed to show number of classes\n",
    "- to split it up equally, used train_test_split from sklearn on the indicies, using stratisfying on the output classes. \n",
    "- Split ratio 80% train 20% test\n",
    "- the overall dataset and the test and training dataset have been plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_split_stats(counts):\n",
    "    total = sum(counts.values())\n",
    "    for k, v in sorted(counts.items()):\n",
    "        print(f\"Class {k} has {v} samples - {v/total*100:.2f}%\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "\n",
    "data = datasets.ImageFolder(root='images', transform=transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]))\n",
    "total_num_classes = Counter(data.targets)\n",
    "print(\"Total data set: \", sum(total_num_classes.values()))\n",
    "print_split_stats(total_num_classes)\n",
    "plt.bar(total_num_classes.keys(), total_num_classes.values(), color='#003f5c')\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, stratify=data.targets)\n",
    "\n",
    "train_dataset = Subset(data, train_indices)\n",
    "num_class_train = Counter(data.targets[i] for i in train_indices)\n",
    "print(\"Train set: \", sum(num_class_train.values()))\n",
    "print_split_stats(num_class_train)\n",
    "plt.bar(num_class_train.keys(), num_class_train.values(), width=0.6, color='#58508d')\n",
    "\n",
    "test_dataset = Subset(data, test_indices)\n",
    "num_class_test = Counter(data.targets[i] for i in test_indices)\n",
    "print(\"Test set: \", sum(num_class_test.values()))\n",
    "print_split_stats(num_class_test)\n",
    "plt.bar(num_class_test.keys(), num_class_test.values(), width=0.4, color='#bc5090')\n",
    "\n",
    "overall_patch = mpatches.Patch(color='#003f5c', label='Overall Dataset')\n",
    "train_patch = mpatches.Patch(color='#58508d', label='Train Split')\n",
    "test_patch = mpatches.Patch(color='#bc5090', label='Test Split')\n",
    "\n",
    "plt.legend(handles=[overall_patch, train_patch, test_patch]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Batches\n",
    "- Much lower number of samples for some of the classes where as other classes have lots of samples. \n",
    "- Don’t want a training batch to contain samples just from the few classes with lots of samples. \n",
    "- Ideally, a training batch should contain represent a good spread of the dataset. \n",
    "- In PyTorch this can be achieved using a weighted random sampler.\n",
    "\n",
    "- Define the weights for each class which would be inversely proportional to the number of samples for each class. USe this to define a sampler, then use this as the sampler for the data loader. \n",
    "- Weighted sampler is not needed for the test set so the sampler is not defined here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data.targets[i] for i in train_indices]\n",
    "class_sample_count = np.array([v for _, v in sorted(num_class_train.items())])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in y_train])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "\n",
    "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.8)\n",
    "cols, rows = 5, 5\n",
    "for i in range(cols * rows):\n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.title(labels[i].item())\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i,:].squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters will only increase as we increase the number of hidden layers. So, the two major disadvantages of using artificial neural networks are:\n",
    "\n",
    "Loses spatial orientation of the image\n",
    "The number of parameters increases drastically\n",
    "https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
    "\n",
    "This is where convolutional neural networks can be really helpful. CNNs help to extract features from the images which may be helpful in classifying the objects in that image. \n",
    "\n",
    "https://medium.com/analytics-vidhya/deep-learning-basics-batch-normalization-ae105f9f537e\n",
    "\n",
    "batch norming ^\n",
    "\n",
    "```\n",
    "[(W−K+2P)/S]+1\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.ConvolutionalLayers = nn.Sequential(\n",
    "        # First convolutional layer\n",
    "        nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, stride=1, padding=1), # output size = B × 12 × 48 x 48\n",
    "        nn.BatchNorm2d(12),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # output size = B × 12 × 24 x 24\n",
    "        nn.Dropout(0.2),\n",
    "\n",
    "        # Second Convolutional layer\n",
    "        nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1), # output size = B × 24 × 24 x 24\n",
    "        nn.BatchNorm2d(24),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2), # output size = B × 24 × 12 x 12\n",
    "        nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            \n",
    "\t\tnn.Linear(in_features=12*12*24, out_features=300),\n",
    "        nn.BatchNorm1d(300),\n",
    "\t\tnn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "\n",
    "        nn.Linear(in_features=300, out_features=180),\n",
    "        nn.BatchNorm1d(180),\n",
    "\n",
    "\t\t# initialize our softmax classifier\n",
    "\t\tnn.Linear(in_features=180, out_features=5),\n",
    "\t\tnn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x has dimensions B x 1 x 48 x 48, B is batch size\n",
    "        x = self.ConvolutionalLayers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.MLP(x)\n",
    "        # Output has dimensions B x 5\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "\n",
    "logps = model(images) # log probabilities\n",
    "loss = loss_func(logps, labels) # calculate the NLL loss\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 20\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        loss = loss_func(output, labels)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(train_dataloader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "\n",
    "check_accuracy(test_dataloader, model)\n",
    "\n",
    "# Run a test batch through the network\n",
    "images, labels = next(iter(test_dataloader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "output = model(images)\n",
    "pred_y = torch.argmax(output, 1)\n",
    "\n",
    "# Display first 25 images with predicted labels\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "for i in range(25):\n",
    "    figure.add_subplot(5, 5, i+1)\n",
    "    plt.title(\"{}/{}\".format(labels[i], pred_y[i]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i,:].squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define some parameters\n",
    "# nz = 100 # Size of z latent vector (i.e. size of generator input)\n",
    "# ngf = 64 # Size of feature maps in generator\n",
    "# feature_maps = 64 # Size of feature maps in discriminator\n",
    "# num_epochs = 5 # Number of training epochs\n",
    "# lr = 0.0002 # Learning rate for optimizers\n",
    "# beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "\n",
    "CGAN_EPOCHS = 20\n",
    "GEN_INPUT_Z = 100\n",
    "\n",
    "CGAN_data = datasets.ImageFolder(root='images', transform=transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                                                                transforms.ToTensor(),\n",
    "                                                                                transforms.Resize((48, 48)), \n",
    "                                                                                transforms.Normalize(0.5,0.5)]))\n",
    "                                                                            \n",
    "CGAN_data.targets = [target-1 for target in CGAN_data.targets]\n",
    "tree_indicies = [i for i in range(len(CGAN_data)) if CGAN_data.targets[i] != -1]\n",
    "\n",
    "CGAN_subset = Subset(CGAN_data, tree_indicies)\n",
    "num_class_gan_train = Counter(CGAN_data.targets[i] for i in tree_indicies)\n",
    "print(\"Train set: \", sum(num_class_gan_train.values()))\n",
    "# print_split_stats(num_class_gan_train)\n",
    "\n",
    "cgan_y_train = [CGAN_data.targets[i] for i in tree_indicies]\n",
    "cgan_class_sample_count = np.array([v for _, v in sorted(num_class_gan_train.items())])\n",
    "cgan_weight = 1. / cgan_class_sample_count\n",
    "cgan_samples_weight = np.array([cgan_weight[t-1] for t in cgan_y_train])\n",
    "cgan_samples_weight = torch.from_numpy(cgan_samples_weight)\n",
    "\n",
    "CGAN_sampler = WeightedRandomSampler(cgan_samples_weight.type('torch.DoubleTensor'), len(cgan_samples_weight))\n",
    "CGAN_dataloader = DataLoader(CGAN_subset, batch_size=BATCH_SIZE, sampler=CGAN_sampler, drop_last=True)\n",
    "images, labels = next(iter(CGAN_dataloader))\n",
    "\n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.8)\n",
    "cols, rows = 5, 5\n",
    "for i in range(cols * rows):\n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.title(labels[i].item())\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i,:].squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class GeneratorModel(nn.Module):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(GeneratorModel, self).__init__()\n",
    "    input_dim = GEN_INPUT_Z + 10\n",
    "    output_dim = 2304\n",
    "    self.label_embedding = nn.Embedding(10, 10)\n",
    "    self.hidden_layer1 = nn.Sequential(\n",
    "    nn.Linear(input_dim, 256),\n",
    "    nn.LeakyReLU(0.2)\n",
    "    )\n",
    "    self.hidden_layer2 = nn.Sequential(\n",
    "    nn.Linear(256, 512),\n",
    "    nn.LeakyReLU(0.2)\n",
    "    )\n",
    "    self.hidden_layer3 = nn.Sequential(\n",
    "    nn.Linear(512, 1024),\n",
    "    nn.LeakyReLU(0.2)\n",
    "    )\n",
    "    self.output_layer = nn.Sequential(\n",
    "    nn.Linear(1024, output_dim),\n",
    "    nn.Tanh()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x, labels):\n",
    "    c = self.label_embedding(labels)\n",
    "    x = torch.cat([x,c], 1)\n",
    "    output = self.hidden_layer1(x)\n",
    "    output = self.hidden_layer2(output)\n",
    "    output = self.hidden_layer3(output)\n",
    "    output = self.output_layer(output)\n",
    "    return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorModel(nn.Module):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(DiscriminatorModel, self).__init__()\n",
    "    input_dim = 2304 + 10\n",
    "    output_dim = 1\n",
    "    self.label_embedding = nn.Embedding(10, 10)\n",
    "    self.hidden_layer1 = nn.Sequential(\n",
    "    nn.Linear(input_dim, 1024),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Dropout(0.3)\n",
    "    )\n",
    "    self.hidden_layer2 = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Dropout(0.3)\n",
    "    )\n",
    "    self.hidden_layer3 = nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Dropout(0.3)\n",
    "    )\n",
    "    self.output_layer = nn.Sequential(\n",
    "    nn.Linear(256, output_dim),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x, labels):\n",
    "    c = self.label_embedding(labels)\n",
    "    x = torch.cat([x, c], 1)\n",
    "    output = self.hidden_layer1(x)\n",
    "    output = self.hidden_layer2(output)\n",
    "    output = self.hidden_layer3(output)\n",
    "    output = self.output_layer(output)\n",
    "    return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = DiscriminatorModel()\n",
    "generator = GeneratorModel()\n",
    "discriminator.to(device);\n",
    "generator.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_idx in range(CGAN_EPOCHS):\n",
    "    G_loss = []\n",
    "    D_loss = []\n",
    "    for batch_idx, data_input in enumerate(CGAN_dataloader):\n",
    "        # Generator\n",
    "        noise = torch.randn(BATCH_SIZE,GEN_INPUT_Z).to(device)\n",
    "        fake_labels = torch.randint(0, 10, (BATCH_SIZE,)).to(device)\n",
    "        generated_data = generator(noise, fake_labels) # batch_size X 784\n",
    "\n",
    "        \n",
    "        # Discriminator\n",
    "        true_data = data_input[0].view(BATCH_SIZE, 2304).to(device) # batch_size X 784\n",
    "        digit_labels = data_input[1].to(device) # batch_size\n",
    "        true_labels = torch.ones(BATCH_SIZE).to(device)\n",
    "        \n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        discriminator_output_for_true_data = discriminator(true_data, digit_labels).view(BATCH_SIZE)\n",
    "        true_discriminator_loss = loss(discriminator_output_for_true_data, true_labels)\n",
    "\n",
    "        discriminator_output_for_generated_data = discriminator(generated_data.detach(), fake_labels).view(BATCH_SIZE)\n",
    "        generator_discriminator_loss = loss(\n",
    "            discriminator_output_for_generated_data, torch.zeros(BATCH_SIZE).to(device)\n",
    "        )\n",
    "        discriminator_loss = (\n",
    "            true_discriminator_loss + generator_discriminator_loss\n",
    "        ) / 2\n",
    "        \n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        D_loss.append(discriminator_loss.data.item())\n",
    "        \n",
    "        \n",
    "        # Generator\n",
    "\n",
    "        generator_optimizer.zero_grad()\n",
    "        # It's a choice to generate the data again\n",
    "        generated_data = generator(noise, fake_labels) # batch_size X 784\n",
    "        discriminator_output_on_generated_data = discriminator(generated_data, fake_labels).view(BATCH_SIZE)\n",
    "        generator_loss = loss(discriminator_output_on_generated_data, true_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        G_loss.append(generator_loss.data.item())\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch_idx), CGAN_EPOCHS, torch.mean(torch.FloatTensor(D_loss)), torch.mean(torch.FloatTensor(G_loss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discriminator.state_dict(), 'q4_discrim_weights.pkl')\n",
    "torch.save(generator.state_dict(), 'q4_gen_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "img_list = []\n",
    "\n",
    "viz_z = torch.zeros((BATCH_SIZE, GEN_INPUT_Z), device=device)\n",
    "viz_noise = torch.randn(BATCH_SIZE, GEN_INPUT_Z, device=device)\n",
    "nrows = BATCH_SIZE // 4\n",
    "viz_label = torch.LongTensor(np.array([num for _ in range(nrows) for num in range(4)])).to(device)\n",
    "print(viz_label)\n",
    "\n",
    "with torch.no_grad():\n",
    "    viz_sample = generator(viz_noise, viz_label).cpu()\n",
    "    img_list.append(vutils.make_grid(viz_sample, normalize=True))\n",
    "\n",
    "# Plot the fake images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/simple-intro-to-conditional-gans-with-torchfusion-and-pytorch-404264a3267c\n",
    "https://medium.com/analytics-vidhya/step-by-step-implementation-of-conditional-generative-adversarial-networks-54e4b47497d6\n",
    "https://colab.research.google.com/drive/1w05w89D6I9ytJyEXBeELN3JLkV-MAT3i?authuser=2#scrollTo=2IIIGl6Ft1xA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "generator.eval()\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "cols, rows = 5, 5\n",
    "for label in range(4):\n",
    "    z = torch.randn(BATCH_SIZE, GEN_INPUT_Z, device=device)\n",
    "    labels = torch.LongTensor(np.array([num for _ in range(16) for num in range(4)])).to(device)\n",
    "    print(z.shape, labels.shape)\n",
    "    \n",
    "    images = generator(z,labels)\n",
    "    for i in range(8):\n",
    "        figure.add_subplot(10, 8, label*8+i+1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(images[i,:].cpu().detach().squeeze().reshape(48, 48), cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
