{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems 3: Probabilistic and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Regression and classification (42 marks)\n",
    "1. Train and evaluate a least squares linear regression model predicting the value of\n",
    "variable D from variables A, B and C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is needed to use matplotlib in Jupyter notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def least_squares_regression(data, train_size, pipeline):\n",
    "    # separates D as the target variable\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "\n",
    "    # Split the data into training/testing sets\n",
    "    X_train = X[:-train_size]\n",
    "    X_test = X[-train_size:]\n",
    "\n",
    "    # Split the targets into training/testing sets\n",
    "    y_train = y[:-train_size]\n",
    "    y_test = y[-train_size:]\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "    print(\"Coefficients of A, B & C:\", pipeline.named_steps['LinearRegression'].coef_)\n",
    "\n",
    "    # The intercept\n",
    "    print(\"Intercept:\", pipeline.named_steps['LinearRegression'].intercept_)\n",
    "\n",
    "    # The mean squared error\n",
    "    print('Mean squared error: %.2f'\n",
    "        % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # The coefficient of determination: \n",
    "    #   1 is perfect prediction\n",
    "    #   0 is as good as always predicting the mean output value (using the training data).\n",
    "    #   negative values are for a model that is worse than just predicting the mean.\n",
    "    print('Coefficient of determination (R2): %.2f'\n",
    "        % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- QUESTION 1 (a) --------------------------------\n",
      "Coefficients of A, B & C: [-2.94826258e+13  5.23720052e+13 -4.71336538e+13]\n",
      "Intercept: -524172862739197.44\n",
      "Mean squared error: 0.43\n",
      "Coefficient of determination (R2): 0.55\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 32 + \" QUESTION 1 (a) \" + \"-\" * 32)\n",
    "\n",
    "# Loads the dataset (skipping the headers) and runs basic linear least squares regression.\n",
    "data = np.genfromtxt('data.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "pipeline = Pipeline([('LinearRegression', LinearRegression())])\n",
    "least_squares_regression(data=data, pipeline=pipeline, train_size=75)\n",
    "\n",
    "print(\"-\" * 76)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repeat the above task after carrying out in turn data normalisation, data scaling and\n",
    "their combination, and evaluate the benefits of each of these 3 types of data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- QUESTION 1 (b)--------------------------------\n",
      "Normalizer:\n",
      "Coefficients of A, B & C: [10.32618043 53.79900775 11.90608955]\n",
      "Intercept: -53.56647174364234\n",
      "Mean squared error: 11.98\n",
      "Coefficient of determination (R2): -11.48\n",
      "\n",
      "Scaler:\n",
      "Coefficients of A, B & C: [0.2139024 0.2139024 0.2139024]\n",
      "Intercept: 1.8055500000000004\n",
      "Mean squared error: 0.65\n",
      "Coefficient of determination (R2): 0.32\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 32 + \" QUESTION 1 (b)\" + \"-\" * 32)\n",
    "\n",
    "# 1. Performs Data Normalisation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "print(\"Normalizer:\")\n",
    "data = np.genfromtxt('data.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "minmax_scaled_pipeline = Pipeline([('Normalizer', Normalizer()), ('LinearRegression', LinearRegression())])\n",
    "least_squares_regression(data=data, pipeline=minmax_scaled_pipeline, train_size=75)\n",
    "\n",
    "# 2. Performs Data Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standard Scaler\n",
    "print(\"\\nScaler:\")\n",
    "data = np.genfromtxt('data.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "std_scaled_pipeline = Pipeline([('Standard Scaler', StandardScaler()), ('LinearRegression', LinearRegression())])\n",
    "least_squares_regression(data=data, pipeline=std_scaled_pipeline, train_size=75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try to outperform the best result of the previous step by using regularisation (e.g. L1,\n",
    "L2 or Elastic Net). Show how any parameter values are tuned and evaluate the benefits of\n",
    "regularisation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Add a set of suitable basis functions to the original data and train a linear regression\n",
    "with an appropriate type of regularisation to find out which of the new basis functions bring\n",
    "benefits. Explain briefly (in no more than 4 sentences) your reasoning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implement an appropriate automated procedure that will train all of the above models\n",
    "and select the model expected to perform best on unseen data with the same distribution as your\n",
    "training data. You need to include a code tile at the end of this section of your Jupyter notebook\n",
    "that attempts to test your final choice of model on a data set stored in a file unseendata.csv\n",
    "and compute $R^2$ for it. The file will have exactly the same format as file data.csv, including\n",
    "the header, but possibly a different overall number of rows. This means you can use a renamed\n",
    "copy of data.csv to debug that part of your code, and to produce the corresponding content\n",
    "for your PDF file (in order to demonstrate that this part of the code is in working order).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Starting with the data in data.csv, find the median value of variable D. Replace all\n",
    "values up to and including the median value with 0, and all values greater than that with 1. Treat\n",
    "the resulting values of D as class labels to train and evaluate a classifier based on logistic\n",
    "regression that takes variables A, B and C as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample accuracy = 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(2)\n",
    "train_size = 75\n",
    "\n",
    "# Loads the dataset (skipping the headers) and shuffles the data.\n",
    "data = np.genfromtxt('data.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Replaces the last column with 1 if the value is greater than the median, 0 otherwise.\n",
    "median = np.median(data[:, 3])\n",
    "data[:, 3] = np.where(data[:, 3] > median, 1, 0)\n",
    "\n",
    "# separates D as the class variable\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "X_train = X[:-train_size]\n",
    "X_test = X[-train_size:]\n",
    "\n",
    "# Split the classifications into training/testing sets\n",
    "y_train = y[:-train_size]\n",
    "y_test = y[-train_size:]\n",
    "\n",
    "# Runs logistic regression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "logisticRegr.predict(X_test)\n",
    "\n",
    "# Use score method to get out of sample accuracy of the model (2 decimal places)\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(\"Out of sample accuracy =\", score.round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Principal Component Analysis (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "\n",
    "data = np.loadtxt('data.csv',delimiter=',',skiprows=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "# extend the table with 6 additional column consisting of the product of each pair of the original 4 variables A, B, C and D\n",
    "for i, j in combinations(range(4), 2):\n",
    "    scaled_data = np.hstack((scaled_data, np.atleast_2d(np.multiply(scaled_data[:, i], scaled_data[:, j])).T))\n",
    "\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(scaled_data)\n",
    "newData = pca.fit_transform(scaled_data)\n",
    "\n",
    "NewTotVar = 0\n",
    "for i, dim in enumerate(('PC1','PC2','PC3','PC4')):\n",
    "  Var2 = np.var(newData[:,i])\n",
    "  NewTotVar += Var2\n",
    "\n",
    "  print('Here is the sample variance for the {0} dimension'.format(dim))\n",
    "  \n",
    "  print('var = {0}\\n\\n'.format(Var2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
